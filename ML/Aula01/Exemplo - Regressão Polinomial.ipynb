{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Exemplo - Regressão Polinomial.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pe-lLjBpQ8xy","colab_type":"text"},"source":["# PolynomialFeatures\n","\n","Como a documentação do sklearn não estava muito clara, achei melhor esclarecer como funciona a regressão polinomial.\n","\n","Suponha que temos uma base de dados $D$ que possui variáveis independentes $A, B$ e variável dependente $T$. Uma observação dessa base é dada por $(a_0, b_0)$, com valor $t_0$. \n","\n","\n","* Uma aproximação linear de $t_0$ (suponha que os valores  $\\beta_i$ foram encontrados utilizando algum algoritmo como mínimos quadrados) pode ser dada por:\n","\n","  $t^{(1)}_0=f^{(1)}(a_0, b_0)= \\beta^{(1)}_0 + \\beta^{(1)}_1 a_0 + \\beta^{(1)}_2 b_0 $\n","\n","* Uma aproximação quadrática (polinômio de segundo grau) da função pode ser vista da forma:\n","\n","  $t^{(2)}_0=f^{(2)}(a_0, b_0)= \\beta^{(2)}_0 + \\beta^{(2)}_1 a_0 + \\beta^{(2)}_2 b_0 + \\beta^{(2)}_3 a_0^2 + \\beta^{(2)}_4 b_0^2 + \\beta^{(2)}_5 a_0 b_0$\n","\n","* Note que podemos encarar \"aproximar via polinômio de segundo grau\" como \"aproximar via função linear\" se considerarmos $a_0^2$, $b_0^2$ e $a_0b_0$ como novas variáveis, $c_0, d_0$ e $e_0$ no problema:\n","\n","  $t^{(3)}_0=f^{(3)}(a_0, b_0, c_0, d_0, e_0)= \\beta^{(3)}_0 + \\beta^{(3)}_1 a_0 + \\beta^{(3)}_2 b_0 + \\beta^{(3)}_3 c_0 + \\beta^{(3)}_4 d_0 + \\beta^{(3)}_5 e_0$\n","\n","* Basta que realizemos a transformação de $(a, b)$ para $(a, b, c, d, e)$, com $c=a^2, d=b^2$ e $e=ab$.\n","\n","\n","---\n","\n","\n","Felizmente a classe **PolynomialFeatures** é bastante útil nesse momento, pois ela nos permite realizar essa transformação facilmente, de forma que consigamos utilizar a mesma sintaxe utilizado anteriormente para adaptar nosso modelo linear.\n","\n","Basta inicializar um polinômio e transformar o conjunto de dados, por exemplo:\n","\n","```python\n","p = PolynomialFeatures(degree=grau)\n","X_Transf = p.fit_transform(X)\n","```\n","\n","E agora ao invés de adaptar nosso modelo linear ao conjunto X, adaptamos ao conjunto X_Transf.\n"]},{"cell_type":"code","metadata":{"id":"iI0gl-e6OWML","colab_type":"code","outputId":"79bf4385-8767-4d57-fc14-a7bc7db81a59","executionInfo":{"status":"ok","timestamp":1584708249926,"user_tz":180,"elapsed":866,"user":{"displayName":"Felipe Padula Sanches","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT8grSHu6fa2Tb5GFr1k7Qp0y7Dt5Ni4IxBC86Ow=s64","userId":"18187017750627346096"}},"colab":{"base_uri":"https://localhost:8080/","height":146}},"source":["from sklearn.preprocessing import PolynomialFeatures\n","import numpy as np\n","\n","#3 observações de 2 variáveis independentes e 1 variável dependente (ultima coluna)\n","# Esse conjunto foi gerado como f(x0, x1) = 3 + 4x1 - 2x0x1 + x0^2\n","data = np.array([[1, 2, 8],\n","              [3, 4, 4],\n","              [0, 0, 3],\n","              [1, 1, 6],\n","              [-1, 4, 28],\n","              [5, 6, -8]])\n","\n","X = data[:, :-1]\n","y = data[:, -1]\n","# Utilizando um polinomio de grau 3\n","p = PolynomialFeatures(degree=2)\n","\n","X_Transf = p.fit_transform(X)\n","print(p.get_feature_names())\n","print(X_Transf)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['1', 'x0', 'x1', 'x0^2', 'x0 x1', 'x1^2']\n","[[ 1.  1.  2.  1.  2.  4.]\n"," [ 1.  3.  4.  9. 12. 16.]\n"," [ 1.  0.  0.  0.  0.  0.]\n"," [ 1.  1.  1.  1.  1.  1.]\n"," [ 1. -1.  4.  1. -4. 16.]\n"," [ 1.  5.  6. 25. 30. 36.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Sk9ob9Dh69CR","colab_type":"text"},"source":["Portanto o realizar regressão linear no conjunto transformado (data_Transf) corresponde a realizar regressão polinomial de grau 2 no conjunto original (data)"]},{"cell_type":"code","metadata":{"id":"4DoqfZWc8i6V","colab_type":"code","outputId":"bdeceabf-6e33-42a8-cc8f-718052d63b8f","executionInfo":{"status":"ok","timestamp":1584708272414,"user_tz":180,"elapsed":1901,"user":{"displayName":"Felipe Padula Sanches","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT8grSHu6fa2Tb5GFr1k7Qp0y7Dt5Ni4IxBC86Ow=s64","userId":"18187017750627346096"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from sklearn.linear_model import LinearRegression\n","\n","# Note que nesse caso colocamos fit_intercept = False pois já capturamos o \n","# termo idependente ao fazermos a transformação anterior\n","lin = LinearRegression(fit_intercept=False)\n","lin.fit(X_Transf, y)\n","\n","print(lin.coef_.round(2))\n","print(lin.score(X_Transf, y))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[ 3. -0.  4.  1. -2. -0.]\n","1.0\n"],"name":"stdout"}]}]}